---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 I am Zhangzhen Zhu (Êú±Âº†Ëµà), a Ph.D. Candidate at ZheJiang University, with the State Key Laboratory of Industrial Control Technology,
College of Control Science and Engineering. My research interests lie in the areas of robust and nonlinear control, in particular, with emphasis on Adaptive High Order Sliding Mode Control/Observer, Time-Varying Input Delay Systems/ Nonlinear Model Predictive Control, Discrete-Time Event-Triggered Control and their applications to complex real systems, including unmanned aerial vehicles and brushless DC motors (see my Publication list for details). 

 In addition to my academic pursuits, I also possess extensive experience in the development of complex Mechatronic systems from scratch (see my Projects list for details), including mechanical system realization, circuit system customization, system modeling and identification, and efficient code deployment. I believe that the familiarity with the hardware systems will effectively enhance control engineers' comprehension of the theoretical research, and to a certain extent, eliminate vacuous mathematical academic research in the control society.
 
 I am dedicated to producing high-quality research and making meaningful contributions to both academics and industrial societies, and looking forward to making progress together with an excellent team.




# üìñ Educations
- *2018.09 - 2023.07 (now)*, PhD Candidate, College of Control Science and Engineering, Zhejiang University, Hangzhou. 
- *2014.09 - 2018.06*, Undergraduate, Chang Kong Honors College, Nanjing University of Aeronautics and Astronautics, Nanjing.




# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun



# üíª Projects
<section id="projects" class="home-section wg-portfolio  ">
 <div class="home-section-bg ">
   
 </div>
  <div class="container">

  
    <div class="row  justify-content-center">
    
      
        <div class="section-heading col-12 mb-3 text-center">
          <h1 class="mb-0">My popular open-source projects</h1>
          
        </div>
      
    
  

    












<div class="col-12 ">

  

  

    

    
    
    
    
      
      
        
      
    

    <span class="d-none default-project-filter">*</span>

    
    
    <div class="project-toolbar">
      <div class="project-filters">
        <div class="btn-toolbar d-flex justify-content-center">
          <div class="btn-group flex-wrap">
            
              
              
              
                
                  
                
              
              <a href="#" data-filter="*" class="btn btn-primary btn-lg active">All</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-immesh" class="btn btn-primary btn-lg">ImMesh</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-r3live" class="btn btn-primary btn-lg">R3LIVE</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-fast-lio" class="btn btn-primary btn-lg">FAST-LIO</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-loam_livox" class="btn btn-primary btn-lg">loam_livox</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-r2live" class="btn btn-primary btn-lg">R2LIVE</a>
            
          </div>
        </div>
      </div>
    </div>
    
  

  <div class="isotope projects-container row js-layout-row project-showcase" style="position: relative; height: 2937.68px;">

    
    

    
    
    
      
    
    
    
    
      
    

    
    
    
    
    

    
      
        
          <div class="col-12 isotope-item js-id-immesh js-id-lidar-slam js-id-multi-sensor-fusion js-id-lider-inertial js-id-3d-reconstruction" style="position: absolute; left: 0px; top: 0px;">
        
        





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0"><a href="/project/proj_immesh/">üÜïImMesh: An Immediate LiDAR Localization and Meshing Framework</a></div>

      
      <div class="article-style">
        <strong>ImMesh</strong> is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of <strong>simultaneous localization and meshing</strong> in real-time. ImMesh comprises four tightly-coupled modules: <em>receiver</em>, <em>localization</em>, <em>meshing</em>, and <em>broadcaster</em>. The <em>localization</em> module utilizes the prepossessed sensor data from the <em>receiver</em>, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our <em>meshing</em> module takes the registered LiDAR scan for <strong>incrementally reconstructing the triangle mesh on the fly</strong>. Finally, the real-time odometry, map, and mesh are published via our <em>broadcaster</em>.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/uploads/ImMesh.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/project/proj_immesh/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/hku-mars/ImMesh" target="_blank" rel="noopener">
    Code on Github (‚òÖ 0.2KüÜï)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://www.bilibili.com/video/BV1AG4y1177z" target="_blank" rel="noopener">
    Video</a>


      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      

      
        
        <a href="/project/proj_immesh/">
          <img src="/project/proj_immesh/featured_hu7f436b084e9af9db6b1e6e2a03bb4ac2_1533052_540x0_resize_q100_h2_lanczos.webp" height="436" width="540" alt="üÜïImMesh: An Immediate LiDAR Localization and Meshing Framework" loading="lazy">
        </a>
      
    </div>
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-r3live js-id-lidar-slam js-id-multi-sensor-fusion js-id-lider-inertial-visual js-id-3d-reconstruction" style="position: absolute; left: 0px; top: 575.555px;">
        
        





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0"><a href="/project/proj_r3live/">R<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 116.9%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></mjx-assistive-mml></mjx-container>LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package</a></div>

      
      <div class="article-style">
        <strong>R<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 119.6%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></mjx-assistive-mml></mjx-container>LIVE</strong> is a versatile and well-engineered system toward various possible applications, which can not only serve as a SLAM system for realtime robotic applications but can also reconstruct the dense, precise, RGB-colored 3D maps for applications like surveying and mapping. In addition, we have developed a series of offline utilities for reconstructing and texturing meshes for various of 3D applications.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/uploads/r3live++.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/project/proj_r3live/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/hku-mars/r3live" target="_blank" rel="noopener">
    Code on Github (‚òÖ 1.4K)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://www.youtube.com/watch?v=j5fT8NE5fdg" target="_blank" rel="noopener">
    Video-1</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://www.youtube.com/watch?v=4rjrrLgL3nk" target="_blank" rel="noopener">
    Video-2</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/ziv-lin/rxlive_handheld" target="_blank" rel="noopener">
    Hardware-Design</a>


      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      

      
        
        <a href="/project/proj_r3live/">
          <img src="/project/proj_r3live/featured_hu9adb7b6ed429abe8c2ea93041c235f8a_2060838_540x0_resize_q100_h2_lanczos.webp" height="519" width="540" alt="R$^3$LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package" loading="lazy">
        </a>
      
    </div>
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-fast-lio js-id-lidar-slam js-id-multi-sensor-fusion js-id-lider-inertial js-id-3d-reconstruction" style="position: absolute; left: 0px; top: 1157.55px;">
        
        





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0"><a href="/project/proj_fastlio/">FAST-LIO2: Fast Direct LiDAR-inertial Odometry</a></div>

      
      <div class="article-style">
        <strong>FAST-LIO</strong> (Fast LiDAR-Inertial Odometry) is a computationally efficient and robust LiDAR-inertial odometry package. It fuses LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. Our package addresses many key issues: 1) Fast error state iterated Kalman filter (ESIKF) for odometry optimization. 2) Incremental mapping using ikd-Tree, achieve faster speed and over 100Hz LiDAR rate. 3) Without the need for feature extraction, FAST-LIO2 can support many types of LiDAR including spinning (Velodyne, Ouster) and solid-state (Livox Avia, Horizon, MID-70) LiDARs, and can be easily extended to support more LiDARs.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/uploads/Fast_LIO_2.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/project/proj_fastlio/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/hku-mars/fast_lio" target="_blank" rel="noopener">
    Code on Github (‚òÖ 1.4K)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://youtu.be/2OvjGnxszf8" target="_blank" rel="noopener">
    Video</a>


      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      

      
        
        <a href="/project/proj_fastlio/">
          <img src="/project/proj_fastlio/featured_hu0dcff6e0a0a468db9565d3c7e7ce5e41_707484_540x0_resize_q100_h2_lanczos.webp" height="543" width="540" alt="FAST-LIO2: Fast Direct LiDAR-inertial Odometry" loading="lazy">
        </a>
      
    </div>
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-r2live js-id-lidar-slam js-id-multi-sensor-fusion js-id-lider-inertial-visual js-id-3d-reconstruction" style="position: absolute; left: 0px; top: 1802.41px;">
        
        





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0"><a href="/project/proj_r2live/">R<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 116.9%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state Estimator and mapping</a></div>

      
      <div class="article-style">
        <strong>R<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 119.6%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>LIVE</strong> is a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses the measurement from the LiDAR, inertial sensor, visual camera to achieve robust, accurate state estimation. Taking advantage of measurement from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environment of different scale.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/uploads/r2live.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/project/proj_r2live/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/hku-mars/r2live" target="_blank" rel="noopener">
    Code on Github (‚òÖ 0.6K)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://www.youtube.com/watch?v=9lqRHmlN_MA" target="_blank" rel="noopener">
    Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/ziv-lin/rxlive_handheld" target="_blank" rel="noopener">
    Hardware-Design</a>


      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      

      
        
        <a href="/project/proj_r2live/">
          <img src="/project/proj_r2live/featured_hu12300ba225464ed1120b3bcb23a7c579_1470136_540x0_resize_q100_h2_lanczos.webp" height="446" width="540" alt="R$^2$LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state Estimator and mapping" loading="lazy">
        </a>
      
    </div>
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-loam_livox js-id-lidar-slam js-id-3d-reconstruction" style="position: absolute; left: 0px; top: 2351.98px;">
        
        





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0"><a href="/project/proj_loam_livox/">LOAM_Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR</a></div>

      
      <div class="article-style">
        <strong>Loam-Livox</strong> is a robust, low drift, and real time odometry and mapping package for <a href="https://www.livoxtech.com/" target="_blank" rel="noopener"><em>Livox LiDARs</em></a>, significant low cost and high performance LiDARs that are designed for massive industrials uses. Our package address many key issues: feature extraction and selection in a very limited FOV, robust outliers rejection, moving objects filtering, and motion distortion compensation. In addition, we also integrate other features like parallelable pipeline, point cloud management using cells and maps, loop closure, utilities for maps saving and reload, etc. To know more about the details, please refer to our related paper:)
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/uploads/loam_livox.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/project/proj_loam_livox/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/hku-mars/loam_livox" target="_blank" rel="noopener">
    Code on Github (‚òÖ 1.2K)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://youtu.be/WHbbtU-Q9-k" target="_blank" rel="noopener">
    Video-1</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://youtu.be/Uq8rUEk-XnI" target="_blank" rel="noopener">
    Video-2</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/ziv-lin/My_solidworks/tree/master/livox_handhold" target="_blank" rel="noopener">
    Hardware-Design</a>


      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      

      
        
        <a href="/project/proj_loam_livox/">
          <img src="/project/proj_loam_livox/featured_hu8b7664fe3a5b10448f6ba6ebc243edb3_323844_540x0_resize_q100_h2_lanczos.webp" height="512" width="540" alt="LOAM_Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR" loading="lazy">
        </a>
      
    </div>
  </div>
</div>

      </div>
    

  </div>
</div>


  
    </div>
  

  </div>
</section>




# üíª Internships
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='../images/zhuzhangzhen.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
- *2016.10 - 2017.06*, [Lorem](https://github.com/), NanJing.

For more info
------
More info about configuring academicpages can be found in [the guide](https://academicpages.github.io/markdown/). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful.
